# Redis分布式锁算法`RedLock`的安全性讨论

  分布式锁是在所有分布式系统和面试中绕不开的话题，我们已知的zookeeper、redis甚至mysql都可以用来做分布式锁，今天我们来谈谈redis分布式锁实现的算法`redLock`的安全性问题。



##  基础回顾

再展开讲redis分布式锁之前我们先花几分钟来聊聊基础只是，如果对分布式锁和redis分布式锁非常了解的可以直接跳过。

- ### 何为分布式锁

  《Designing Data-Intensive Applications》的作者Martin Kleppmann如此解释分布式锁：

  > The purpose of a lock is to ensure that among several nodes that might try to do the same piece of work, only one actually does it (at least only one at a time). That work might be to write some data to a shared storage system, to perform some computation, to call some external API, or suchlike. At a high level, there are two reasons why you might want a lock in a distributed application: for efficiency or for correctness

  总之分布式锁就是在你需要确保在可能尝试执行同一工作的多个节点中，只有一个实际上可以执行（一次至少一次）。但是分布式锁拥有很昂贵的性能开销这也符合CAP原则，所以如果你的系统对可用性要求很高的话那么就不要去使用分布式锁了。

- ## Redis如何实现分布式锁

  - ### 简单的实现

  Redis的作者antirez在文章中指出要实现一个最低保障的分布式锁的算法必须要具备以下三个特征：

  > 1. 安全属性（Safety property）: 独享（相互排斥）。在任意一个时刻，只有一个客户端持有锁。
  > 2. 活性A(Liveness property A): 无死锁。即便持有锁的客户端崩溃（crashed)或者网络被分裂（gets partitioned)，锁仍然可以被获取。
  > 3. 活性B(Liveness property B): 容错。 只要大部分Redis节点都活着，客户端就可以获取和释放锁.

  按照这个思路他也提供了一个最简单的用redis实现分布式锁的办法：

  ​    在Redis中set一个key，这个key有一个失效时间，以保证锁最终会被自动释放掉（这个对应特性2）。当客户端释放资源(解锁）的时候，会删除掉这个key。

  从表面上看，似乎效果还不错，但是这里有一个问题：这个架构中存在一个严重的单点失败问题。如果Redis挂了怎么办？你可能会说，可以通过增加一个slave节点解决这个问题。但这通常是行不通的。这样做，我们不能实现资源的独享,因为Redis的主从同步通常是异步的。

  在这种场景（主从结构）中存在明显的竞态:

  1. 客户端A从master获取到锁
  2. 在master将锁同步到slave之前，master宕掉了。
  3. slave节点被晋级为master节点
  4. 客户端B取得了同一个资源被客户端A已经获取到的另外一个锁。**安全失效！**

  有时候程序就是这么巧，比如说正好一个节点挂掉的时候，多个客户端同时取到了锁。如果你可以接受这种小概率错误，但是这个做法仍然有一个问题：可能没有获取到锁的客户端可以解开这把锁，只需要执行下del命令即可，所以上面的方法不是完全正确的分布式锁的解决方案，让我们来看看正确的方案：

  - ###  单Redis实例实现分布式锁的正确方法

    在尝试克服上述单实例设置的限制之前，让我们先讨论一下在这种简单情况下实现分布式锁的正确做法，获取锁使用命令:` SET resource_name my_random_value NX PX 30000` ；这个命令仅在不存在key的时候才能被执行成功（NX选项），并且这个key有一个30秒的自动失效时间（PX属性）。这个key的值是“my_random_value”(一个随机值），这个值在所有的客户端必须是唯一的，所有同一key的获取者（竞争者）这个值都不能一样。

  value的值必须是随机数主要是为了更安全的释放锁，释放锁的时候使用脚本告诉Redis:只有key存在并且存储的值和我指定的值一样才能告诉我删除成功。可以通过以下Lua脚本实现：

  ```lua
  if redis.call("get",KEYS[1]) == ARGV[1] then
      return redis.call("del",KEYS[1])
  else
      return 0
  end
  ```

  使用这种方式释放锁可以避免删除别的客户端获取成功的锁。举个例子：客户端A取得资源锁，但是紧接着被一个其他操作阻塞了，当客户端A运行完毕其他操作后要释放锁时，原来的锁早已超时并且被Redis自动释放，并且在这期间资源锁又被客户端B再次获取到。如果仅使用DEL命令将key删除，那么这种情况就会把客户端B的锁给删除掉。使用Lua脚本就不会存在这种情况，因为脚本仅会删除value等于客户端A的value的key（value相当于客户端的一个签名）。

  这个随机字符串应该怎么设置？Autirez给出的建议之一就是: `是把以毫秒为单位的unix时间和客户端ID拼接起来，理论上不是完全安全，但是在多数情况下可以满足需求.` 总之要保证多个客户端生成的随机值不一致就可以了，当然因为可以人为的去修改客户端上的时间所以这个方法并不是百分之百安全的，但是大多数情况是正确的。

  - ### 集群下的Redis分布式锁算法—RedLock

    在Redis的分布式环境中，我们假设有N个Redis master。这些节点完全互相独立，不存在主从复制或者其他集群协调机制。之前我们已经描述了在Redis单实例下怎么安全地获取和释放锁。我们确保将在每（N)个实例上使用此方法获取和释放锁。在这个样例中，我们假设有5个Redis master节点，这是一个比较合理的设置，所以我们需要在5台机器上面或者5台虚拟机上面运行这些实例，这样保证他们不会同时都宕掉。

  为了取到锁，客户端应该执行以下操作:

  1. 获取当前Unix时间，以毫秒为单位。
  2. 依次尝试从N个实例，使用相同的key和随机值获取锁。在步骤2，当向Redis设置锁时,客户端应该设置一个网络连接和响应超时时间，这个超时时间应该小于锁的失效时间。例如你的锁自动失效时间为10秒，则超时时间应该在5-50毫秒之间。这样可以避免服务器端Redis已经挂掉的情况下，客户端还在死死地等待响应结果。如果服务器端没有在规定时间内响应，客户端应该尽快尝试另外一个Redis实例。
  3. 客户端使用当前时间减去开始获取锁时间（步骤1记录的时间）就得到获取锁使用的时间。当且仅当从大多数（这里是3个节点）的Redis节点都取到锁，并且使用的时间小于锁失效时间时，锁才算获取成功。
  4. 如果取到了锁，key的真正有效时间等于有效时间减去获取锁所使用的时间（步骤3计算的结果）。
  5. 如果因为某些原因，获取锁失败（*没有*在至少N/2+1个Redis实例取到锁或者取锁时间已经超过了有效时间），客户端应该在所有的Redis实例上进行解锁（即便某些Redis实例根本就没有加锁成功）。

  总结一下RedLock算法的原理就是，假设我们有5个redis节点，我们去用单节点获取锁的方式向每个节点都申请锁，只有在N/2+1个节点都申请成功了的话才算是获取到了锁（N/2+1的意思是超过半数），如果你只在2个节点上获取到了锁然后剩下节点都获取失败了的话那你锁就获取失败了，你必须把你获取到的锁释放掉然后从头开始获取锁，而且这也解决了单节点下分布式锁的问题：客户端A获取到了ABC三个节点额，然后随后AB节点挂掉了，客户端B再获取锁最多只能获取到DE两个节点的锁还是没有超过半数，所以获取失败。

  Autirez自己在文章中分析了这一算法的安全性：

  > 这个算法安全么？我们可以从不同的场景讨论一下。
  >
  > 让我们假设客户端从大多数Redis实例取到了锁。所有的实例都包含同样的key，并且key的有效时间也一样。然而，key肯定是在不同的时间被设置上的，所以key的失效时间也不是精确的相同。我们假设第一个设置的key时间是T1(开始向第一个server发送命令前时间），最后一个设置的key时间是T2(得到最后一台server的答复后的时间），我们可以确认，第一个server的key至少会存活 `MIN_VALIDITY=TTL-(T2-T1)-CLOCK_DRIFT`。所有其他的key的存活时间，都会比这个key时间晚，所以可以肯定，所有key的失效时间至少是MIN_VALIDITY。
  >
  > 当大部分实例的key被设置后，其他的客户端将不能再取到锁，因为至少N/2+1个实例已经存在key。所以，如果一个锁被（客户端）获取后，客户端自己也不能再次申请到锁(违反互相排斥属性）。
  >
  > 然而我们也想确保，当多个客户端同时抢夺一个锁时不能两个都成功。
  >
  > 如果客户端在获取到大多数redis实例锁，使用的时间接近或者已经大于失效时间，客户端将认为锁是失效的锁，并且将释放掉已经获取到的锁，所以我们只需要在有效时间范围内获取到大部分锁这种情况。在上面已经讨论过有争议的地方，在`MIN_VALIDITY`时间内，将没有客户端再次取得锁。所以只有一种情况，多个客户端会在相同时间取得N/2+1实例的锁，那就是取得锁的时间大于失效时间（TTL time)，这样取到的锁也是无效的

  他也在文章最后分析了活性争议和性能等，总之Autirez觉得这个算法没有问题。



## RedLock的安全性争议

​	最先提出争议的是分布式系统的大牛—《Designing Data-Intensive Applications》的作者Martin Kleppmann，他写了篇文章来反驳这个算法：https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html

我总结了下这篇文章，马丁主要有以下的观点：

- ### RedLock的用途

  ​	其实刚刚在介绍RedLock的时候就会发现，五个Redis节点是不是太多了呢？如果仅出于提高效率的目的使用锁，则不用承担Redlock的成本和复杂性，无需运行5台Redis服务器并检查大多数锁来获取锁。最好只使用一个Redis实例，最好是异步复制到slave，以防master崩溃。但是使用单个Redis实例，那么如果Redis节点突然断电或发生其他问题，就会像上文说的一样会丢失锁，安全性就失效了。但是，如果只是将锁用作效率优化，并且单节点宕掉这种情况不会经常发生，那其实可以忽略。Redis大放异彩是这种“没什么大不了”的场景。至少如果依赖单个Redis实例，那么查看系统的每个人都清楚锁是近似的，并且仅用于非关键目的。另一方面，具有5个副本和多数投票的Redlock算法乍一看，似乎适用于锁定对于*正确性*很重要的情况，但是如果两个不同的节点同时认为它们持有相同的锁，则这是一个严重的错误。

- ### RedLock的漏洞

  ​	假设系统有五个Redis节点（A，B，C，D和E）和两个客户端（1和2）。

  第一种情况：

    如果Redis节点之一上的时钟向前跳怎么办？

  1. 客户端1获取节点A，B，C的锁定。由于网络问题，无法访问D和E。
  2. 节点C上的时钟向前跳，导致锁过期。
  3. 客户端2获取节点C，D，E的锁定。由于网络问题，无法访问A和B。
  4. 现在，客户1和2都认为他们持有该锁。

  如果C在将锁保留到磁盘之前崩溃并立即重新启动，则可能会发生类似的问题。因此，Redlock文档建议将崩溃节点的重新启动至少延迟]最长寿命的锁的生存时间。但是，重新启动延迟又依赖于时间的合理准确测量，并且如果时钟跳变将失败

  第二种情况：

  1. 客户端1请求锁定节点A，B，C，D，E。
  2. 在发送对客户端1的响应时，客户端1因为自身代码原因阻塞或者是因为GC引起的stw阻塞，于是过了锁的超时时间。
  3. 锁在所有Redis节点上失效。
  4. 客户端2获取对节点A，B，C，D，E的锁定。
  5. 客户端1完成GC，并接收来自Redis节点的响应，表明它已成功获取了锁（在进程暂停时，它们已保存在客户端1的内核网络缓冲区中）。
  6. 现在，客户1和2都认为他们持有该锁。

  请注意，即使Redis是用C编写的，因此没有GC，但这在这里也无济于事：任何*客户端*可能会遇到GC暂停的系统都存在此问题。您只能通过阻止客户端1在客户端2获得锁之后执行该锁下的任何操作（例如使用上述防护方法）来确保此安全。

  较长的网络延迟会产生与过程暂停相同的效果。这可能取决于你设置的TCP的超时时间-如果将超时时间大大短于Redis TTL，则可能会忽略延迟的网络数据包，但是为了确保这一点，我们必须详细研究TCP。此外，随着超时，我们又回到了时间测量的准确性！

- ### 用栅栏保护锁安全

  马丁在第三段中提出了一个方案——用token来做fencing：
  
  ​	客户端1获得了锁，并获得了33的token，但随后k开始阻塞了，锁超时作废。客户端2获得锁，获得34的token（假设token是递增的），然后将其insert的操作（包括令牌34）发送到存储服务。稍后，客户端1恢复并将其写操作发送到存储服务，当然也包含了33的token。但是，存储服务器只会处理更大一点的token，所以存储服务只会执行客户端B的token为34的请求不会响应客户端A的token33的请求。
  
  ![](https://ftp.bmp.ovh/imgs/2021/03/61e015c812699fa6.png)
  
  ​	但是以上方案有一个问题：存储服务需要强识别token不允许低位的token来操作，那么问题是谁来确定这个token的唯一性？那又是另外一个问题了；马丁的观点是zookeeper可以使用`zxid` 或znode版本号用作为token并且可以保证其安全性唯一性，但是RedLock算法没有这种机制。并且马丁也不知道如何更改Redlock算法以开始生成隔离令牌。它使用的唯一随机值不能提供所需的单调性。仅在一个Redis节点上保留一个计数器是不够的，因为该节点可能会失败。在多个节点上保留计数器将意味着它们将不同步。可能需要一个共识算法才能生成围栏令牌。（如果仅增加一个计数器很简单。）



即使Autirez自己后面又写了一篇文章来反驳，但是我认为他的反驳非常无力，并不能有力的来证明马丁的论点是错的。有兴趣的大家可以自己去看一下：http://antirez.com/news/101

## 个人总结

​	马丁认为RedLock算法是在一切假设成立的情况下比如分布式系统的时间一致、系统内部一切运行良好才能是没有漏洞的分布式锁算法，但是这恰好是分布式系统的最大问题——你没法保证这一切都是完美的。其次RedLock太“重”了，无论是对性能开销还是服务器负担来说，也许你说可以用单节点的Redis分布式锁啊，但是单节点下的Redis分布式锁就算再一切正常的情况也不能百分之百保证是没有问问题的，这就有了一个悖论：

​	众所周知分布式系统满足CAP理论即一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）最多只能满足两个，如果我们使用了Redis分布式锁那么我们就是牺牲了可用性来满足一致性和分区容错性也就说CP系统，那么我们需要的就是我们的系统是强一致性的即使说他只有千万分之一的情况发生错误（想象一下如果有一个可能流动上亿的资金系统你和你老板说这个可能有千万分之会出问题，老板会不会换个人来做？）我们也是不能接受的，因为我们都牺牲了可用性了。那你可能会问了如果我愿意牺牲一致性呢？那就绕回来了如果你只满足可用性和分区容错性的话那你就不会需要分布式锁了，最多也是使用单节点的Redis分布式锁而不需要使用RedLock算法了。总之CP系统用RedLock是无法保证百分之百的一致性的，AP系统又没有必要用分布锁。

​	也就是说RedLock并不是一个优秀的分布式锁解决方案，马丁更推荐zookeeper来作为分布式锁，他不会产生RedLock算法的问题，本文就不再赘述zookeeper作为分布式锁的方案了，分享一个用zookeeper来作为分布式锁的论文有兴趣的可以看一下：https://pdos.csail.mit.edu/6.824/papers/zookeeper.pdf

​	当然了如果你的系统只是一个普通的业务系统只是单纯的用单节点Redis分布式锁方案的我觉得也没啥问题，但是如果你的系统对数据一致性要求很高的话比如说银行系统交易系统等，那么建议你最好不要用Redission来做分布式锁了。



- 参考：

  https://redis.io/topics/distlock -Autirez

  https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html -Martin Kleppmann

​		https://en.wikipedia.org/wiki/CAP_theorem